{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "0f73ca0c-acfc-4160-932f-ac425e965f9b",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Obtaining file:///C:/Users/singh/Downloads/gym/gym-env\n",
      "Requirement already satisfied: gym in c:\\users\\singh\\documents\\gym (from gym-env==1.0.0) (0.19.0)\n",
      "Requirement already satisfied: numpy>=1.18.0 in c:\\users\\singh\\anaconda3\\lib\\site-packages (from gym->gym-env==1.0.0) (1.20.1)\n",
      "Requirement already satisfied: cloudpickle>=1.2.0 in c:\\users\\singh\\anaconda3\\lib\\site-packages (from gym->gym-env==1.0.0) (1.6.0)\n",
      "Installing collected packages: gym-env\n",
      "  Attempting uninstall: gym-env\n",
      "    Found existing installation: gym-env 1.0.0\n",
      "    Uninstalling gym-env-1.0.0:\n",
      "      Successfully uninstalled gym-env-1.0.0\n",
      "  Running setup.py develop for gym-env\n",
      "Successfully installed gym-env\n"
     ]
    }
   ],
   "source": [
    "!pip install --user --upgrade --editable gym-env"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "e04793dd-7a68-4fb0-8924-64429fce8f29",
   "metadata": {},
   "outputs": [],
   "source": [
    "import warnings ; warnings.filterwarnings('ignore')\n",
    "import os\n",
    "\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.nn.functional as F\n",
    "import torch.optim as optim\n",
    "\n",
    "import numpy as np\n",
    "from IPython.display import display\n",
    "from collections import namedtuple, deque\n",
    "import matplotlib.pyplot as plt\n",
    "import matplotlib.pylab as pylab\n",
    "from itertools import cycle, count\n",
    "from textwrap import wrap\n",
    "\n",
    "import matplotlib\n",
    "import subprocess\n",
    "import os.path\n",
    "import tempfile\n",
    "import random\n",
    "import base64\n",
    "import pprint\n",
    "import glob\n",
    "import time\n",
    "import json\n",
    "import sys\n",
    "import gym\n",
    "import io\n",
    "import os\n",
    "import gc\n",
    "\n",
    "from gym import wrappers\n",
    "from subprocess import check_output\n",
    "from IPython.display import HTML\n",
    "\n",
    "LEAVE_PRINT_EVERY_N_SECS = 60\n",
    "ERASE_LINE = '\\x1b[2K'\n",
    "EPS = 1e-6\n",
    "BEEP = lambda: os.system(\"printf '\\a'\")\n",
    "RESULTS_DIR = os.path.join('..', 'results')\n",
    "SEEDS = (12, 34, 56, 78, 90)\n",
    "\n",
    "%matplotlib inline"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "4e15047f-3168-4d7c-add9-7d940ae346f7",
   "metadata": {},
   "outputs": [],
   "source": [
    "class FCQ(nn.Module):\n",
    "    def __init__(self, \n",
    "                 input_dim, \n",
    "                 output_dim, \n",
    "                 hidden_dims=(32,32), \n",
    "                 activation_fc=F.relu):\n",
    "        super(FCQ, self).__init__()\n",
    "        self.activation_fc = activation_fc\n",
    "\n",
    "        self.input_layer = nn.Linear(input_dim, hidden_dims[0])\n",
    "\n",
    "        self.hidden_layers = nn.ModuleList()\n",
    "        for i in range(len(hidden_dims)-1):\n",
    "            hidden_layer = nn.Linear(hidden_dims[i], hidden_dims[i+1])\n",
    "            self.hidden_layers.append(hidden_layer)\n",
    "        self.output_layer = nn.Linear(hidden_dims[-1], output_dim)\n",
    "\n",
    "        device = \"cpu\"\n",
    "        if torch.cuda.is_available():\n",
    "            device = \"cuda:0\"\n",
    "        self.device = torch.device(device)\n",
    "        self.to(self.device)\n",
    "        \n",
    "    def _format(self, state):\n",
    "        x = state\n",
    "        if not isinstance(x, torch.Tensor):\n",
    "            x = torch.tensor(x, \n",
    "                             device=self.device, \n",
    "                             dtype=torch.float32)\n",
    "            x = x.unsqueeze(0)\n",
    "        return x\n",
    "\n",
    "    def forward(self, state):\n",
    "        x = self._format(state)\n",
    "        x = self.activation_fc(self.input_layer(x))\n",
    "        for hidden_layer in self.hidden_layers:\n",
    "            x = self.activation_fc(hidden_layer(x))\n",
    "        x = self.output_layer(x)\n",
    "        return x\n",
    "    \n",
    "    def numpy_float_to_device(self, variable):\n",
    "        variable = torch.from_numpy(variable).float().to(self.device)\n",
    "        return variable\n",
    "    \n",
    "    def load(self, experiences):\n",
    "        states, actions, new_states, rewards, is_terminals = experiences\n",
    "        states = torch.from_numpy(states).float().to(self.device)\n",
    "        actions = torch.from_numpy(actions).long().to(self.device)\n",
    "        new_states = torch.from_numpy(new_states).float().to(self.device)\n",
    "        rewards = torch.from_numpy(rewards).float().to(self.device)\n",
    "        is_terminals = torch.from_numpy(is_terminals).float().to(self.device)\n",
    "        return states, actions, new_states, rewards, is_terminals"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "300adbf5-a33c-4c61-a7f8-f0a6d563bc6d",
   "metadata": {},
   "outputs": [],
   "source": [
    "class GreedyStrategy():\n",
    "    def __init__(self):\n",
    "        self.exploratory_action_taken = False\n",
    "\n",
    "    def select_action(self, model, state):\n",
    "        with torch.no_grad():\n",
    "            q_values = model(state).cpu().detach().data.numpy().squeeze()\n",
    "            return np.argmax(q_values)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "a8cb79ec-5852-4da4-8277-566fe76069ec",
   "metadata": {},
   "outputs": [],
   "source": [
    "class EGreedyStrategy():\n",
    "    def __init__(self, epsilon=0.1):\n",
    "        self.epsilon = epsilon\n",
    "        self.exploratory_action_taken = None\n",
    "\n",
    "    def select_action(self, model, state):\n",
    "        self.exploratory_action_taken = False\n",
    "        with torch.no_grad():\n",
    "            q_values = model(state).cpu().detach().data.numpy().squeeze()\n",
    "\n",
    "        if np.random.rand() > self.epsilon:\n",
    "            action = np.argmax(q_values)\n",
    "        else: \n",
    "            action = np.random.randint(len(q_values))\n",
    "\n",
    "        self.exploratory_action_taken = action != np.argmax(q_values)\n",
    "        return action"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "b7a8ed08-c8e1-4b3c-98b2-f6e6cac74cea",
   "metadata": {},
   "outputs": [],
   "source": [
    "class EGreedyLinearStrategy():\n",
    "    def __init__(self, init_epsilon=1.0, min_epsilon=0.1, decay_steps=20000):\n",
    "        self.t = 0\n",
    "        self.epsilon = init_epsilon\n",
    "        self.init_epsilon = init_epsilon\n",
    "        self.min_epsilon = min_epsilon\n",
    "        self.decay_steps = decay_steps\n",
    "        self.exploratory_action_taken = None\n",
    "        \n",
    "    def _epsilon_update(self):\n",
    "        epsilon = 1 - self.t / self.decay_steps\n",
    "        epsilon = (self.init_epsilon - self.min_epsilon) * epsilon + self.min_epsilon\n",
    "        epsilon = np.clip(epsilon, self.min_epsilon, self.init_epsilon)\n",
    "        self.t += 1\n",
    "        return epsilon\n",
    "\n",
    "    def select_action(self, model, state):\n",
    "        self.exploratory_action_taken = False\n",
    "        with torch.no_grad():\n",
    "            q_values = model(state).cpu().detach().data.numpy().squeeze()\n",
    "\n",
    "        if np.random.rand() > self.epsilon:\n",
    "            action = np.argmax(q_values)\n",
    "        else: \n",
    "            action = np.random.randint(len(q_values))\n",
    "\n",
    "        self.epsilon = self._epsilon_update()\n",
    "        self.exploratory_action_taken = action != np.argmax(q_values)\n",
    "        return action"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "f4086cae-aee4-4a9c-9b77-55a3954afab7",
   "metadata": {},
   "outputs": [],
   "source": [
    "class EGreedyExpStrategy():\n",
    "    def __init__(self, init_epsilon=1.0, min_epsilon=0.1, decay_steps=20000):\n",
    "        self.epsilon = init_epsilon\n",
    "        self.init_epsilon = init_epsilon\n",
    "        self.decay_steps = decay_steps\n",
    "        self.min_epsilon = min_epsilon\n",
    "        self.epsilons = 0.01 / np.logspace(-2, 0, decay_steps, endpoint=False) - 0.01\n",
    "        self.epsilons = self.epsilons * (init_epsilon - min_epsilon) + min_epsilon\n",
    "        self.t = 0\n",
    "        self.exploratory_action_taken = None\n",
    "\n",
    "    def _epsilon_update(self):\n",
    "        self.epsilon = self.min_epsilon if self.t >= self.decay_steps else self.epsilons[self.t]\n",
    "        self.t += 1\n",
    "        return self.epsilon\n",
    "\n",
    "    def select_action(self, model, state):\n",
    "        self.exploratory_action_taken = False\n",
    "        with torch.no_grad():\n",
    "            q_values = model(state).detach().cpu().data.numpy().squeeze()\n",
    "\n",
    "        if np.random.rand() > self.epsilon:\n",
    "            action = np.argmax(q_values)\n",
    "        else:\n",
    "            action = np.random.randint(len(q_values))\n",
    "\n",
    "        self._epsilon_update()\n",
    "        self.exploratory_action_taken = action != np.argmax(q_values)\n",
    "        return action"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "623dbf8e-b6f0-46ea-af72-323fe39025de",
   "metadata": {},
   "outputs": [],
   "source": [
    "class SoftMaxStrategy():\n",
    "    def __init__(self, \n",
    "                 init_temp=1.0, \n",
    "                 min_temp=0.3, \n",
    "                 exploration_ratio=0.8, \n",
    "                 max_steps=25000):\n",
    "        self.t = 0\n",
    "        self.init_temp = init_temp\n",
    "        self.exploration_ratio = exploration_ratio\n",
    "        self.min_temp = min_temp\n",
    "        self.max_steps = max_steps\n",
    "        self.exploratory_action_taken = None\n",
    "        \n",
    "    def _update_temp(self):\n",
    "        temp = 1 - self.t / (self.max_steps * self.exploration_ratio)\n",
    "        temp = (self.init_temp - self.min_temp) * temp + self.min_temp\n",
    "        temp = np.clip(temp, self.min_temp, self.init_temp)\n",
    "        self.t += 1\n",
    "        return temp\n",
    "\n",
    "    def select_action(self, model, state):\n",
    "        self.exploratory_action_taken = False\n",
    "        temp = self._update_temp()\n",
    "\n",
    "        with torch.no_grad():\n",
    "            q_values = model(state).cpu().detach().data.numpy().squeeze()\n",
    "            scaled_qs = q_values/temp\n",
    "            norm_qs = scaled_qs - scaled_qs.max()            \n",
    "            e = np.exp(norm_qs)\n",
    "            probs = e / np.sum(e)\n",
    "            assert np.isclose(probs.sum(), 1.0)\n",
    "\n",
    "        action = np.random.choice(np.arange(len(probs)), size=1, p=probs)[0]\n",
    "        self.exploratory_action_taken = action != np.argmax(q_values)\n",
    "        return action"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "9ff63147-aa50-4333-8a48-86a3dade5e5f",
   "metadata": {},
   "outputs": [],
   "source": [
    "class ReplayBuffer():\n",
    "    def __init__(self, \n",
    "                 max_size=10000, \n",
    "                 batch_size=64):\n",
    "        self.ss_mem = np.empty(shape=(max_size), dtype=np.ndarray)\n",
    "        self.as_mem = np.empty(shape=(max_size), dtype=np.ndarray)\n",
    "        self.rs_mem = np.empty(shape=(max_size), dtype=np.ndarray)\n",
    "        self.ps_mem = np.empty(shape=(max_size), dtype=np.ndarray)\n",
    "        self.ds_mem = np.empty(shape=(max_size), dtype=np.ndarray)\n",
    "\n",
    "        self.max_size = max_size\n",
    "        self.batch_size = batch_size\n",
    "        self._idx = 0\n",
    "        self.size = 0\n",
    "    \n",
    "    def store(self, sample):\n",
    "        s, a, r, p, d = sample\n",
    "        self.ss_mem[self._idx] = s\n",
    "        self.as_mem[self._idx] = a\n",
    "        self.rs_mem[self._idx] = r\n",
    "        self.ps_mem[self._idx] = p\n",
    "        self.ds_mem[self._idx] = d\n",
    "        \n",
    "        self._idx += 1\n",
    "        self._idx = self._idx % self.max_size\n",
    "\n",
    "        self.size += 1\n",
    "        self.size = min(self.size, self.max_size)\n",
    "\n",
    "    def sample(self, batch_size=None):\n",
    "        if batch_size == None:\n",
    "            batch_size = self.batch_size\n",
    "\n",
    "        idxs = np.random.choice(\n",
    "            self.size, batch_size, replace=False)\n",
    "        experiences = np.vstack(self.ss_mem[idxs]), \\\n",
    "                      np.vstack(self.as_mem[idxs]), \\\n",
    "                      np.vstack(self.rs_mem[idxs]), \\\n",
    "                      np.vstack(self.ps_mem[idxs]), \\\n",
    "                      np.vstack(self.ds_mem[idxs])\n",
    "        return experiences\n",
    "\n",
    "    def __len__(self):\n",
    "        return self.size"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "71e57752-8cb7-465a-b1da-bd259a44598e",
   "metadata": {},
   "outputs": [],
   "source": [
    "class DDQN():\n",
    "    def __init__(self, \n",
    "                 replay_buffer_fn, \n",
    "                 value_model_fn, \n",
    "                 value_optimizer_fn, \n",
    "                 value_optimizer_lr,\n",
    "                 max_gradient_norm,\n",
    "                 training_strategy_fn,\n",
    "                 evaluation_strategy_fn,\n",
    "                 n_warmup_batches,\n",
    "                 update_target_every_steps):\n",
    "        self.replay_buffer_fn = replay_buffer_fn\n",
    "        self.value_model_fn = value_model_fn\n",
    "        self.value_optimizer_fn = value_optimizer_fn\n",
    "        self.value_optimizer_lr = value_optimizer_lr\n",
    "        self.max_gradient_norm = max_gradient_norm\n",
    "        self.training_strategy_fn = training_strategy_fn\n",
    "        self.evaluation_strategy_fn = evaluation_strategy_fn\n",
    "        self.n_warmup_batches = n_warmup_batches\n",
    "        self.update_target_every_steps = update_target_every_steps\n",
    "\n",
    "    def optimize_model(self, experiences):\n",
    "        states, actions, rewards, next_states, is_terminals = experiences\n",
    "        batch_size = len(is_terminals)\n",
    "        \n",
    "        # argmax_a_q_sp = self.target_model(next_states).max(1)[1]\n",
    "        argmax_a_q_sp = self.online_model(next_states).max(1)[1]\n",
    "        q_sp = self.target_model(next_states).detach()\n",
    "        max_a_q_sp = q_sp[\n",
    "            np.arange(batch_size), argmax_a_q_sp].unsqueeze(1)\n",
    "        target_q_sa = rewards + (self.gamma * max_a_q_sp * (1 - is_terminals))\n",
    "        q_sa = self.online_model(states).gather(1, actions)\n",
    "\n",
    "        td_error = q_sa - target_q_sa\n",
    "        value_loss = td_error.pow(2).mul(0.5).mean()\n",
    "        self.value_optimizer.zero_grad()\n",
    "        value_loss.backward()        \n",
    "        torch.nn.utils.clip_grad_norm_(self.online_model.parameters(), \n",
    "                                       self.max_gradient_norm)\n",
    "        self.value_optimizer.step()\n",
    "\n",
    "    def interaction_step(self, state, env):\n",
    "        action = self.training_strategy.select_action(self.online_model, state)\n",
    "        new_state, reward, is_terminal, info = env.step(action)\n",
    "        is_truncated = False\n",
    "        is_failure = is_terminal and not is_truncated\n",
    "        experience = (state, action, reward, new_state, float(is_failure))\n",
    "\n",
    "        self.replay_buffer.store(experience)\n",
    "        self.episode_reward[-1] += reward\n",
    "        self.episode_timestep[-1] += 1\n",
    "        self.episode_exploration[-1] += int(self.training_strategy.exploratory_action_taken)\n",
    "        return new_state, is_terminal\n",
    "    \n",
    "    def update_network(self):\n",
    "        for target, online in zip(self.target_model.parameters(), \n",
    "                                  self.online_model.parameters()):\n",
    "            target.data.copy_(online.data)\n",
    "\n",
    "    def train(self, seed, gamma, \n",
    "              max_minutes, max_episodes, goal_mean_100_reward):\n",
    "        training_start, last_debug_time = time.time(), float('-inf')\n",
    "\n",
    "        self.checkpoint_dir = tempfile.mkdtemp()\n",
    "        self.seed = seed\n",
    "        self.gamma = gamma\n",
    "        \n",
    "        env = gym.make('gym_env:foraging-v0', interval_time=10, total_time=4)\n",
    "        env.seed(self.seed)\n",
    "        torch.manual_seed(self.seed) ; np.random.seed(self.seed) ; random.seed(self.seed)\n",
    "    \n",
    "        nS, nA = 6, env.action_space.n\n",
    "        self.episode_timestep = []\n",
    "        self.episode_reward = []\n",
    "        self.episode_seconds = []\n",
    "        self.evaluation_scores = []        \n",
    "        self.episode_exploration = []\n",
    "        \n",
    "        self.target_model = self.value_model_fn(nS, nA)\n",
    "        self.online_model = self.value_model_fn(nS, nA)\n",
    "        self.update_network()\n",
    "\n",
    "        self.value_optimizer = self.value_optimizer_fn(self.online_model, \n",
    "                                                       self.value_optimizer_lr)\n",
    "\n",
    "        self.replay_buffer = self.replay_buffer_fn()\n",
    "        self.training_strategy = training_strategy_fn()\n",
    "        self.evaluation_strategy = evaluation_strategy_fn() \n",
    "                    \n",
    "        result = np.empty((max_episodes, 5))\n",
    "        result[:] = np.nan\n",
    "        training_time = 0\n",
    "        for episode in range(1, max_episodes + 1):\n",
    "            episode_start = time.time()\n",
    "            \n",
    "            state, is_terminal = env.reset()\n",
    "            self.episode_reward.append(0.0)\n",
    "            self.episode_timestep.append(0.0)\n",
    "            self.episode_exploration.append(0.0)\n",
    "\n",
    "            for step in count():\n",
    "                state, is_terminal = self.interaction_step(state, env)\n",
    "                \n",
    "                min_samples = self.replay_buffer.batch_size * self.n_warmup_batches\n",
    "                if len(self.replay_buffer) > min_samples:\n",
    "                    experiences = self.replay_buffer.sample()\n",
    "                    experiences = self.online_model.load(experiences)\n",
    "                    self.optimize_model(experiences)\n",
    "                \n",
    "                if np.sum(self.episode_timestep) % self.update_target_every_steps == 0:\n",
    "                    self.update_network()\n",
    "\n",
    "                if is_terminal:\n",
    "                    gc.collect()\n",
    "                    break\n",
    "            \n",
    "            # stats\n",
    "            episode_elapsed = time.time() - episode_start\n",
    "            self.episode_seconds.append(episode_elapsed)\n",
    "            training_time += episode_elapsed\n",
    "            evaluation_score, _ = self.evaluate(self.online_model, env)\n",
    "            self.save_checkpoint(episode-1, self.online_model)\n",
    "            \n",
    "            total_step = int(np.sum(self.episode_timestep))\n",
    "            self.evaluation_scores.append(evaluation_score)\n",
    "            \n",
    "            mean_10_reward = np.mean(self.episode_reward[-10:])\n",
    "            std_10_reward = np.std(self.episode_reward[-10:])\n",
    "            mean_100_reward = np.mean(self.episode_reward[-100:])\n",
    "            std_100_reward = np.std(self.episode_reward[-100:])\n",
    "            mean_100_eval_score = np.mean(self.evaluation_scores[-100:])\n",
    "            std_100_eval_score = np.std(self.evaluation_scores[-100:])\n",
    "            lst_100_exp_rat = np.array(\n",
    "                self.episode_exploration[-100:])/np.array(self.episode_timestep[-100:])\n",
    "            mean_100_exp_rat = np.mean(lst_100_exp_rat)\n",
    "            std_100_exp_rat = np.std(lst_100_exp_rat)\n",
    "            \n",
    "            wallclock_elapsed = time.time() - training_start\n",
    "            result[episode-1] = total_step, mean_100_reward, \\\n",
    "                mean_100_eval_score, training_time, wallclock_elapsed\n",
    "            \n",
    "            reached_debug_time = time.time() - last_debug_time >= LEAVE_PRINT_EVERY_N_SECS\n",
    "            reached_max_minutes = wallclock_elapsed >= max_minutes * 60\n",
    "            reached_max_episodes = episode >= max_episodes\n",
    "            reached_goal_mean_reward = mean_100_eval_score >= goal_mean_100_reward\n",
    "            training_is_over = reached_max_minutes or \\\n",
    "                               reached_max_episodes or \\\n",
    "                               reached_goal_mean_reward\n",
    "\n",
    "            elapsed_str = time.strftime(\"%H:%M:%S\", time.gmtime(time.time() - training_start))\n",
    "            debug_message = 'el {}, ep {:04}, ts {:06}, '\n",
    "            debug_message += 'ar 10 {:05.1f}\\u00B1{:05.1f}, '\n",
    "            debug_message += '100 {:05.1f}\\u00B1{:05.1f}, '\n",
    "            debug_message += 'ex 100 {:02.1f}\\u00B1{:02.1f}, '\n",
    "            debug_message += 'ev {:05.1f}\\u00B1{:05.1f}'\n",
    "            debug_message = debug_message.format(\n",
    "                elapsed_str, episode-1, total_step, mean_10_reward, std_10_reward, \n",
    "                mean_100_reward, std_100_reward, mean_100_exp_rat, std_100_exp_rat,\n",
    "                mean_100_eval_score, std_100_eval_score)\n",
    "            print(debug_message, end='\\r', flush=True)\n",
    "            if reached_debug_time or training_is_over:\n",
    "                print(ERASE_LINE + debug_message, flush=True)\n",
    "                last_debug_time = time.time()\n",
    "            if training_is_over:\n",
    "                if reached_max_minutes: print(u'--> reached_max_minutes \\u2715')\n",
    "                if reached_max_episodes: print(u'--> reached_max_episodes \\u2715')\n",
    "                if reached_goal_mean_reward: print(u'--> reached_goal_mean_reward \\u2713')\n",
    "                break\n",
    "                \n",
    "        final_eval_score, score_std = self.evaluate(self.online_model, env, n_episodes=100)\n",
    "        wallclock_time = time.time() - training_start\n",
    "        print('Training complete.')\n",
    "        print('Final evaluation score {:.2f}\\u00B1{:.2f} in {:.2f}s training time,'\n",
    "              ' {:.2f}s wall-clock time.\\n'.format(\n",
    "                  final_eval_score, score_std, training_time, wallclock_time))\n",
    "        env.close() ; del env\n",
    "        self.get_cleaned_checkpoints()\n",
    "        return result, final_eval_score, training_time, wallclock_time\n",
    "    \n",
    "    def evaluate(self, eval_policy_model, eval_env, n_episodes=1):\n",
    "        rs = []\n",
    "        for _ in range(n_episodes):\n",
    "            s, d = eval_env.reset()\n",
    "            rs.append(0)\n",
    "            for _ in count():\n",
    "                a = self.evaluation_strategy.select_action(eval_policy_model, s)\n",
    "                s, r, d, _ = eval_env.step(a)\n",
    "                rs[-1] += r\n",
    "                if d: break\n",
    "        return np.mean(rs), np.std(rs)\n",
    "\n",
    "    def get_cleaned_checkpoints(self, n_checkpoints=5):\n",
    "        try: \n",
    "            return self.checkpoint_paths\n",
    "        except AttributeError:\n",
    "            self.checkpoint_paths = {}\n",
    "\n",
    "        paths = glob.glob(os.path.join(self.checkpoint_dir, '*.tar'))\n",
    "        paths_dic = {int(path.split('.')[-2]):path for path in paths}\n",
    "        last_ep = max(paths_dic.keys())\n",
    "        # checkpoint_idxs = np.geomspace(1, last_ep+1, n_checkpoints, endpoint=True, dtype=np.int)-1\n",
    "        checkpoint_idxs = np.linspace(1, last_ep+1, n_checkpoints, endpoint=True, dtype=np.int)-1\n",
    "\n",
    "        for idx, path in paths_dic.items():\n",
    "            if idx in checkpoint_idxs:\n",
    "                self.checkpoint_paths[idx] = path\n",
    "            else:\n",
    "                os.unlink(path)\n",
    "\n",
    "        return self.checkpoint_paths\n",
    "\n",
    "    def demo_last(self, title='Fully-trained {} Agent', n_episodes=3, max_n_videos=3):\n",
    "        env = self.make_env_fn(**self.make_env_kargs, monitor_mode='evaluation', render=True, record=True)\n",
    "\n",
    "        checkpoint_paths = self.get_cleaned_checkpoints()\n",
    "        last_ep = max(checkpoint_paths.keys())\n",
    "        self.online_model.load_state_dict(torch.load(checkpoint_paths[last_ep]))\n",
    "\n",
    "        self.evaluate(self.online_model, env, n_episodes=n_episodes)\n",
    "        env.close()\n",
    "        data = get_gif_html(env_videos=env.videos, \n",
    "                            title=title.format(self.__class__.__name__),\n",
    "                            max_n_videos=max_n_videos)\n",
    "        del env\n",
    "        return HTML(data=data)\n",
    "\n",
    "    def demo_progression(self, title='{} Agent progression', max_n_videos=5):\n",
    "        env = self.make_env_fn(**self.make_env_kargs, monitor_mode='evaluation', render=True, record=True)\n",
    "\n",
    "        checkpoint_paths = self.get_cleaned_checkpoints()\n",
    "        for i in sorted(checkpoint_paths.keys()):\n",
    "            self.online_model.load_state_dict(torch.load(checkpoint_paths[i]))\n",
    "            self.evaluate(self.online_model, env, n_episodes=1)\n",
    "\n",
    "        env.close()\n",
    "        data = get_gif_html(env_videos=env.videos, \n",
    "                            title=title.format(self.__class__.__name__),\n",
    "                            subtitle_eps=sorted(checkpoint_paths.keys()),\n",
    "                            max_n_videos=max_n_videos)\n",
    "        del env\n",
    "        return HTML(data=data)\n",
    "\n",
    "    def save_checkpoint(self, episode_idx, model):\n",
    "        torch.save(model.state_dict(), \n",
    "                   os.path.join(self.checkpoint_dir, 'model.{}.tar'.format(episode_idx)))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "eebfef0b-aa6d-4630-ac65-441f29b6ddba",
   "metadata": {},
   "outputs": [
    {
     "ename": "ImportError",
     "evalue": "cannot import name 'Foraging0' from 'gym_env.envs.foraging0' (c:\\users\\singh\\downloads\\gym\\gym-env\\gym_env\\envs\\foraging0.py)",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mImportError\u001b[0m                               Traceback (most recent call last)",
      "\u001b[1;32m<ipython-input-12-19490b4da1c3>\u001b[0m in \u001b[0;36m<module>\u001b[1;34m\u001b[0m\n\u001b[0;32m     36\u001b[0m                  update_target_every_steps)\n\u001b[0;32m     37\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m---> 38\u001b[1;33m     \u001b[0mresult\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mfinal_eval_score\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mtraining_time\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mwallclock_time\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0magent\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mtrain\u001b[0m\u001b[1;33m(\u001b[0m \u001b[0mseed\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mgamma\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mmax_minutes\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mmax_episodes\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mgoal_mean_100_reward\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m     39\u001b[0m     \u001b[0mddqn_results\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mappend\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mresult\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     40\u001b[0m     \u001b[0mddqn_agents\u001b[0m\u001b[1;33m[\u001b[0m\u001b[0mseed\u001b[0m\u001b[1;33m]\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0magent\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m<ipython-input-9-552022749c96>\u001b[0m in \u001b[0;36mtrain\u001b[1;34m(self, seed, gamma, max_minutes, max_episodes, goal_mean_100_reward)\u001b[0m\n\u001b[0;32m     66\u001b[0m         \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mgamma\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mgamma\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     67\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m---> 68\u001b[1;33m         \u001b[0menv\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mgym\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mmake\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;34m'gym_env:foraging-v0'\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0minterval_time\u001b[0m\u001b[1;33m=\u001b[0m\u001b[1;36m10\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mtotal_time\u001b[0m\u001b[1;33m=\u001b[0m\u001b[1;36m4\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m     69\u001b[0m         \u001b[0menv\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mseed\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mseed\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     70\u001b[0m         \u001b[0mtorch\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mmanual_seed\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mseed\u001b[0m\u001b[1;33m)\u001b[0m \u001b[1;33m;\u001b[0m \u001b[0mnp\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mrandom\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mseed\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mseed\u001b[0m\u001b[1;33m)\u001b[0m \u001b[1;33m;\u001b[0m \u001b[0mrandom\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mseed\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mseed\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32mc:\\users\\singh\\documents\\gym\\gym\\envs\\registration.py\u001b[0m in \u001b[0;36mmake\u001b[1;34m(id, **kwargs)\u001b[0m\n\u001b[0;32m    196\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    197\u001b[0m \u001b[1;32mdef\u001b[0m \u001b[0mmake\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mid\u001b[0m\u001b[1;33m,\u001b[0m \u001b[1;33m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m--> 198\u001b[1;33m     \u001b[1;32mreturn\u001b[0m \u001b[0mregistry\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mmake\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mid\u001b[0m\u001b[1;33m,\u001b[0m \u001b[1;33m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m    199\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    200\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32mc:\\users\\singh\\documents\\gym\\gym\\envs\\registration.py\u001b[0m in \u001b[0;36mmake\u001b[1;34m(self, path, **kwargs)\u001b[0m\n\u001b[0;32m    104\u001b[0m             \u001b[0mlogger\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0minfo\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;34m\"Making new env: %s\"\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mpath\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    105\u001b[0m         \u001b[0mspec\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mspec\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mpath\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m--> 106\u001b[1;33m         \u001b[0menv\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mspec\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mmake\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m    107\u001b[0m         \u001b[1;31m# We used to have people override _reset/_step rather than\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    108\u001b[0m         \u001b[1;31m# reset/step. Set _gym_disable_underscore_compat = True on\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32mc:\\users\\singh\\documents\\gym\\gym\\envs\\registration.py\u001b[0m in \u001b[0;36mmake\u001b[1;34m(self, **kwargs)\u001b[0m\n\u001b[0;32m     73\u001b[0m             \u001b[0menv\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mentry_point\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m**\u001b[0m\u001b[0m_kwargs\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     74\u001b[0m         \u001b[1;32melse\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m---> 75\u001b[1;33m             \u001b[0mcls\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mload\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mentry_point\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m     76\u001b[0m             \u001b[0menv\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mcls\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m**\u001b[0m\u001b[0m_kwargs\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     77\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32mc:\\users\\singh\\documents\\gym\\gym\\envs\\registration.py\u001b[0m in \u001b[0;36mload\u001b[1;34m(name)\u001b[0m\n\u001b[0;32m     16\u001b[0m \u001b[1;32mdef\u001b[0m \u001b[0mload\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mname\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     17\u001b[0m     \u001b[0mmod_name\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mattr_name\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mname\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0msplit\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;34m\":\"\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m---> 18\u001b[1;33m     \u001b[0mmod\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mimportlib\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mimport_module\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mmod_name\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m     19\u001b[0m     \u001b[0mfn\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mgetattr\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mmod\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mattr_name\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     20\u001b[0m     \u001b[1;32mreturn\u001b[0m \u001b[0mfn\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m~\\anaconda3\\lib\\importlib\\__init__.py\u001b[0m in \u001b[0;36mimport_module\u001b[1;34m(name, package)\u001b[0m\n\u001b[0;32m    125\u001b[0m                 \u001b[1;32mbreak\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    126\u001b[0m             \u001b[0mlevel\u001b[0m \u001b[1;33m+=\u001b[0m \u001b[1;36m1\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m--> 127\u001b[1;33m     \u001b[1;32mreturn\u001b[0m \u001b[0m_bootstrap\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0m_gcd_import\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mname\u001b[0m\u001b[1;33m[\u001b[0m\u001b[0mlevel\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mpackage\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mlevel\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m    128\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    129\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m~\\anaconda3\\lib\\importlib\\_bootstrap.py\u001b[0m in \u001b[0;36m_gcd_import\u001b[1;34m(name, package, level)\u001b[0m\n",
      "\u001b[1;32m~\\anaconda3\\lib\\importlib\\_bootstrap.py\u001b[0m in \u001b[0;36m_find_and_load\u001b[1;34m(name, import_)\u001b[0m\n",
      "\u001b[1;32m~\\anaconda3\\lib\\importlib\\_bootstrap.py\u001b[0m in \u001b[0;36m_find_and_load_unlocked\u001b[1;34m(name, import_)\u001b[0m\n",
      "\u001b[1;32m~\\anaconda3\\lib\\importlib\\_bootstrap.py\u001b[0m in \u001b[0;36m_load_unlocked\u001b[1;34m(spec)\u001b[0m\n",
      "\u001b[1;32m~\\anaconda3\\lib\\importlib\\_bootstrap_external.py\u001b[0m in \u001b[0;36mexec_module\u001b[1;34m(self, module)\u001b[0m\n",
      "\u001b[1;32m~\\anaconda3\\lib\\importlib\\_bootstrap.py\u001b[0m in \u001b[0;36m_call_with_frames_removed\u001b[1;34m(f, *args, **kwds)\u001b[0m\n",
      "\u001b[1;32mc:\\users\\singh\\downloads\\gym\\gym-env\\gym_env\\envs\\__init__.py\u001b[0m in \u001b[0;36m<module>\u001b[1;34m\u001b[0m\n\u001b[1;32m----> 1\u001b[1;33m \u001b[1;32mfrom\u001b[0m \u001b[0mgym_env\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0menvs\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mforaging0\u001b[0m \u001b[1;32mimport\u001b[0m \u001b[0mForaging0\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m      2\u001b[0m \u001b[1;32mfrom\u001b[0m \u001b[0mgym_env\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0menvs\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mforaging1\u001b[0m \u001b[1;32mimport\u001b[0m \u001b[0mForaging1\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m      3\u001b[0m \u001b[1;32mfrom\u001b[0m \u001b[0mgym_env\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0menvs\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mforaging2\u001b[0m \u001b[1;32mimport\u001b[0m \u001b[0mForaging2\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;31mImportError\u001b[0m: cannot import name 'Foraging0' from 'gym_env.envs.foraging0' (c:\\users\\singh\\downloads\\gym\\gym-env\\gym_env\\envs\\foraging0.py)"
     ]
    }
   ],
   "source": [
    "ddqn_results = []\n",
    "ddqn_agents, best_ddqn_agent_key, best_eval_score = {}, None, float('-inf')\n",
    "for seed in SEEDS:\n",
    "    environment_settings = {\n",
    "        'env_name': 'gym_env:foraging-v0',\n",
    "        'gamma': 1.00,\n",
    "        'max_minutes': 20,\n",
    "        'max_episodes': 10000,\n",
    "        'goal_mean_100_reward': 5000\n",
    "    }\n",
    "\n",
    "    value_model_fn = lambda nS, nA: FCQ(nS, nA, hidden_dims=(512,128))\n",
    "    value_optimizer_fn = lambda net, lr: optim.RMSprop(net.parameters(), lr=lr)\n",
    "    value_optimizer_lr = 0.0005\n",
    "    max_gradient_norm = float('inf')\n",
    "\n",
    "    training_strategy_fn = lambda: EGreedyExpStrategy(init_epsilon=1.0,  \n",
    "                                                      min_epsilon=0.3, \n",
    "                                                      decay_steps=20000)\n",
    "    evaluation_strategy_fn = lambda: GreedyStrategy()\n",
    "\n",
    "    replay_buffer_fn = lambda: ReplayBuffer(max_size=50000, batch_size=64)\n",
    "    n_warmup_batches = 5\n",
    "    update_target_every_steps = 10\n",
    "    \n",
    "    env_name, gamma, max_minutes, \\\n",
    "    max_episodes, goal_mean_100_reward = environment_settings.values()\n",
    "    agent = DDQN(replay_buffer_fn, \n",
    "                 value_model_fn, \n",
    "                 value_optimizer_fn, \n",
    "                 value_optimizer_lr,\n",
    "                 max_gradient_norm,\n",
    "                 training_strategy_fn,\n",
    "                 evaluation_strategy_fn,\n",
    "                 n_warmup_batches,\n",
    "                 update_target_every_steps)\n",
    "\n",
    "    result, final_eval_score, training_time, wallclock_time = agent.train( seed, gamma, max_minutes, max_episodes, goal_mean_100_reward)\n",
    "    ddqn_results.append(result)\n",
    "    ddqn_agents[seed] = agent\n",
    "    if final_eval_score > best_eval_score:\n",
    "        best_eval_score = final_eval_score\n",
    "        best_ddqn_agent_key = seed\n",
    "ddqn_results = np.array(ddqn_results)\n",
    "_ = BEEP()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b4f01a4c-d06d-42c3-b568-b27d1431f796",
   "metadata": {},
   "outputs": [],
   "source": [
    "ddqn_max_t, ddqn_max_r, ddqn_max_s, ddqn_max_sec, ddqn_max_rt = np.max(ddqn_results, axis=0).T\n",
    "ddqn_min_t, ddqn_min_r, ddqn_min_s, ddqn_min_sec, ddqn_min_rt = np.min(ddqn_results, axis=0).T\n",
    "ddqn_mean_t, ddqn_mean_r, ddqn_mean_s, ddqn_mean_sec, ddqn_mean_rt = np.mean(ddqn_results, axis=0).T\n",
    "ddqn_x = np.arange(np.max((len(ddqn_mean_s))))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "a51fa272-a577-4fc9-a633-4cc5023136f7",
   "metadata": {},
   "outputs": [
    {
     "ename": "NameError",
     "evalue": "name 'ddqn_max_r' is not defined",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mNameError\u001b[0m                                 Traceback (most recent call last)",
      "\u001b[1;32m<ipython-input-11-15a31150cf29>\u001b[0m in \u001b[0;36m<module>\u001b[1;34m\u001b[0m\n\u001b[0;32m      2\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m      3\u001b[0m \u001b[1;31m# ddqn\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m----> 4\u001b[1;33m \u001b[0maxs\u001b[0m\u001b[1;33m[\u001b[0m\u001b[1;36m0\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mplot\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mddqn_max_r\u001b[0m\u001b[1;33m,\u001b[0m \u001b[1;34m'y'\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mlinewidth\u001b[0m\u001b[1;33m=\u001b[0m\u001b[1;36m1\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m      5\u001b[0m \u001b[0maxs\u001b[0m\u001b[1;33m[\u001b[0m\u001b[1;36m0\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mplot\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mddqn_min_r\u001b[0m\u001b[1;33m,\u001b[0m \u001b[1;34m'y'\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mlinewidth\u001b[0m\u001b[1;33m=\u001b[0m\u001b[1;36m1\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m      6\u001b[0m \u001b[0maxs\u001b[0m\u001b[1;33m[\u001b[0m\u001b[1;36m0\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mplot\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mddqn_mean_r\u001b[0m\u001b[1;33m,\u001b[0m \u001b[1;34m'y'\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mlabel\u001b[0m\u001b[1;33m=\u001b[0m\u001b[1;34m'ddqn'\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mlinewidth\u001b[0m\u001b[1;33m=\u001b[0m\u001b[1;36m2\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;31mNameError\u001b[0m: name 'ddqn_max_r' is not defined"
     ]
    },
    {
     "data": {
      "image/png": "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\n",
      "text/plain": [
       "<Figure size 1440x2160 with 5 Axes>"
      ]
     },
     "metadata": {
      "needs_background": "light"
     },
     "output_type": "display_data"
    }
   ],
   "source": [
    "fig, axs = plt.subplots(5, 1, figsize=(20,30), sharey=False, sharex=True)\n",
    "\n",
    "# ddqn\n",
    "axs[0].plot(ddqn_max_r, 'y', linewidth=1)\n",
    "axs[0].plot(ddqn_min_r, 'y', linewidth=1)\n",
    "axs[0].plot(ddqn_mean_r, 'y', label='ddqn', linewidth=2)\n",
    "axs[0].fill_between(ddqn_x, ddqn_min_r, ddqn_max_r, facecolor='y', alpha=0.3)\n",
    "\n",
    "axs[1].plot(ddqn_max_s, 'y', linewidth=1)\n",
    "axs[1].plot(ddqn_min_s, 'y', linewidth=1)\n",
    "axs[1].plot(ddqn_mean_s, 'y', label='ddqn', linewidth=2)\n",
    "axs[1].fill_between(ddqn_x, ddqn_min_s, ddqn_max_s, facecolor='y', alpha=0.3)\n",
    "\n",
    "axs[2].plot(ddqn_max_t, 'y', linewidth=1)\n",
    "axs[2].plot(ddqn_min_t, 'y', linewidth=1)\n",
    "axs[2].plot(ddqn_mean_t, 'y', label='ddqn', linewidth=2)\n",
    "axs[2].fill_between(ddqn_x, ddqn_min_t, ddqn_max_t, facecolor='y', alpha=0.3)\n",
    "\n",
    "axs[3].plot(ddqn_max_sec, 'y', linewidth=1)\n",
    "axs[3].plot(ddqn_min_sec, 'y', linewidth=1)\n",
    "axs[3].plot(ddqn_mean_sec, 'y', label='ddqn', linewidth=2)\n",
    "axs[3].fill_between(ddqn_x, ddqn_min_sec, ddqn_max_sec, facecolor='y', alpha=0.3)\n",
    "\n",
    "axs[4].plot(ddqn_max_rt, 'y', linewidth=1)\n",
    "axs[4].plot(ddqn_min_rt, 'y', linewidth=1)\n",
    "axs[4].plot(ddqn_mean_rt, 'y', label='ddqn', linewidth=2)\n",
    "axs[4].fill_between(ddqn_x, ddqn_min_rt, ddqn_max_rt, facecolor='y', alpha=0.3)\n",
    "\n",
    "# ALL\n",
    "axs[0].set_title('Moving Avg Reward (Training)')\n",
    "axs[1].set_title('Moving Avg Reward (Evaluation)')\n",
    "axs[2].set_title('Total Steps')\n",
    "axs[3].set_title('Training Time')\n",
    "axs[4].set_title('Wall-clock Time')\n",
    "plt.xlabel('Episodes')\n",
    "axs[0].legend(loc='upper left')\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c7847ecd-6f88-4c44-8902-ee697572b2ad",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.8"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
