{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "!pip install --user --upgrade --editable gym-env"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Restart Kernel"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import gym\n",
    "from gym import spaces\n",
    "import numpy as np\n",
    "from numpy import random as rand\n",
    "import random\n",
    "import math\n",
    "import matplotlib.pyplot as plt\n",
    "import sys"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "env = gym.make('gym_env:foraging-v102') #rich patch poor patch environment"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "noEpisodes=1000\n",
    "mp=100\n",
    "mh=500\n",
    "noEnvs=10\n",
    "g=0.7"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def decay(initial, final, stepstoDecay,noEpisodes, type):\n",
    "    x=np.zeros(noEpisodes)\n",
    "    \n",
    "    a=stepstoDecay\n",
    "    k = np.log(initial/final)/(a-1)\n",
    "    for e in range(a):\n",
    "        if type==\"linear\":\n",
    "            x[e]=(initial-(e)*(initial-final)/(a-1))\n",
    "        else:\n",
    "            x[e]=(initial*np.exp(-(e)*k))\n",
    "    for e in range(a,noEpisodes):\n",
    "        x[e]=final     \n",
    "    return x\n",
    "   "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "alpha = decay(1,0.1,noEpisodes,noEpisodes,\"exp\")/100000\n",
    "\n",
    "epsilon = decay(1,0.0001,600,noEpisodes,\"exp\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def actionSelect(p,h,Q,epsilon):\n",
    "    if random.random()>epsilon:\n",
    "        a = np.argmax(Q[p][h])\n",
    "        pos = []\n",
    "        for i in range(len(Q[p][h])):\n",
    "            if(Q[p][h][i]==Q[p][h][a]):\n",
    "                pos.append(i)\n",
    "        a = random.choice(pos)\n",
    "    else:\n",
    "        a = random.randint(0,1)\n",
    "    return a\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def q_learning(env,mp,mh,gamma,alpha0,epsilon0,noEpisodes):\n",
    "    #initialisation\n",
    "    P=np.zeros(noEpisodes)\n",
    "    rho=0\n",
    "    Q=np.zeros((mp,mh,env.action_space.n))\n",
    "    Qs=np.zeros((noEpisodes,mp,mh,env.action_space.n))\n",
    "    #Algorithm\n",
    "    R=np.zeros((noEpisodes))\n",
    "    for e in range (noEpisodes):\n",
    "        h,done=env.reset()\n",
    "        p=0\n",
    "        alpha=alpha0[e]\n",
    "        epsilon=epsilon0[e]\n",
    "        # print(t)\n",
    "        r=0\n",
    "        while not done:\n",
    "            a=actionSelect(p,h,Q,epsilon)\n",
    "            h_next,reward,done = env.step(a)\n",
    "            if a==0 :\n",
    "            # if h_next==0:\n",
    "                p_next=p+1\n",
    "            else:\n",
    "                p_next=p\n",
    "            td_target=reward\n",
    "            r+=reward\n",
    "            if not done:\n",
    "                td_target=td_target+gamma*np.amax(Q[p_next][h_next])\n",
    "            td_error = td_target - Q[p][h][a]-rho\n",
    "            Q[p][h][a]=Q[p][h][a]+alpha*td_error\n",
    "            p,h=p_next,h_next\n",
    "        P[e]=rho\n",
    "        rho=(e*rho+(r/240))/(e+1)\n",
    "            \n",
    "        R[e]=r #bookkeeping for rewards\n",
    "        Qs[e]=Q\n",
    "    # V,pi=np.amax(Q,axis=2),np.argmax(Q,axis=2)\n",
    "    return R\n",
    "   "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "R1=[]\n",
    "for i in range(noEnvs):\n",
    "    # print(i)\n",
    "\n",
    "    env1 = gym.make('gym_env:foraging-v102', interval_time = 10, decision_time=\"V\")\n",
    "\n",
    "    env1.seed(i+50)\n",
    "\n",
    "    random.seed(i+50)\n",
    "    r1=q_learning(env1,mp,mh,g,alpha,epsilon,noEpisodes)\n",
    "\n",
    "    R1.append(r1)\n",
    "\n",
    "R1=np.array(R1)\n",
    "\n",
    "A1=np.average(R1,axis=0)\n",
    "\n",
    "F=np.zeros(noEpisodes)\n",
    "\n",
    "for e in range(noEpisodes):\n",
    "    F[e]=np.mean(A1[e-99:e+1])\n",
    "\n",
    "plt.plot(F, color = 'blue',label=\"Uniform (0.6-1.4) sec\")\n",
    "plt.xlabel(\"Episodes\")\n",
    "plt.ylabel(\"Rewards\")\n",
    "plt.title(\"Travel Time =10 seconds\")\n",
    "plt.savefig('2D R .pdf')\n",
    "plt.show()\n",
    "plt.close()\n",
    "\n",
    "'''\n",
    "    Here we plot for decision time = 3 seconds\n",
    "\n",
    "\n",
    "'''\n",
    "U1=[]\n",
    "\n",
    "for i in range(noEnvs):\n",
    "    # print(i)\n",
    "    env1 = gym.make('gym_env:foraging-v102', interval_time = 3, decision_time=\"V\")\n",
    "    \n",
    "    env1.seed(i+50)\n",
    "\n",
    "    random.seed(i+50)\n",
    "    r1=q_learning(env1,mp,mh,g,alpha,epsilon,noEpisodes)\n",
    "\n",
    "    U1.append(r1)\n",
    "\n",
    "U1=np.array(U1)\n",
    "\n",
    "A1=np.average(U1,axis=0)\n",
    "\n",
    "for e in range(noEpisodes):\n",
    "    F[e]=np.mean(A1[e-99:e+1])\n",
    "\n",
    "plt.plot(F, color = 'blue',label=\"Uniform (0.6-1.4) sec\")\n",
    "\n",
    "\n",
    "plt.xlabel(\"Episodes\")\n",
    "plt.ylabel(\"Rewards\")\n",
    "plt.title(\"Travel Time = 3 seconds\")\n",
    "plt.savefig('2D R tt=3.pdf')\n",
    "plt.show()\n",
    "plt.close()\n",
    "\n"
   ]
  }
 ],
 "metadata": {
  "language_info": {
   "name": "python"
  },
  "orig_nbformat": 4
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
